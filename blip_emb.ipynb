{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steven/miniconda3/envs/tasti/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tasti\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import BlipProcessor, BlipPreTrainedModel, BlipConfig, BlipVisionModel, BlipTextModel\n",
    "from transformers.models.blip.modeling_blip import BlipImageTextMatchingModelOutput, BlipTextVisionModelOutput\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load \"./cache/embeddings.npy\"\n",
    "def load_embeddings():\n",
    "    embeddings = np.load(\"./cache/embeddings.npy\")\n",
    "    return embeddings\n",
    "\n",
    "# load \"./y_true.npy\"\n",
    "def load_y_true():\n",
    "    y_true = np.load(\"./y_true.npy\")\n",
    "    return y_true\n",
    "\n",
    "embeddings = load_embeddings()\n",
    "y_true = load_y_true()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 256)\n",
      "(2500,)\n"
     ]
    }
   ],
   "source": [
    "print(embeddings.shape)\n",
    "print(y_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BlipForImageTextRetrieval(BlipPreTrainedModel):\n",
    "    config_class = BlipConfig\n",
    "\n",
    "    def __init__(self, config: BlipConfig):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.vision_model = BlipVisionModel(config.vision_config)\n",
    "\n",
    "        self.text_encoder = BlipTextModel(config.text_config, add_pooling_layer=False)\n",
    "\n",
    "        # vision projection layer\n",
    "        self.vision_proj = nn.Linear(config.vision_config.hidden_size, config.image_text_hidden_size)\n",
    "\n",
    "        # text projection layer\n",
    "        self.text_proj = nn.Linear(config.text_config.hidden_size, config.image_text_hidden_size)\n",
    "\n",
    "        # image text matching head\n",
    "        self.itm_head = nn.Linear(config.text_config.hidden_size, 2)\n",
    "\n",
    "        self.decoder_pad_token_id = (\n",
    "            config.text_config.pad_token_id\n",
    "            if not hasattr(config, \"decoder_pad_token_id\")\n",
    "            else config.decoder_pad_token_id\n",
    "        )\n",
    "        self.decoder_start_token_id = (\n",
    "            config.text_config.bos_token_id\n",
    "            if not hasattr(config, \"decoder_start_token_id\")\n",
    "            else config.decoder_start_token_id\n",
    "        )\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self) -> nn.Module:\n",
    "        return self.vision_model.embeddings.patch_embedding\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        pixel_values: torch.FloatTensor,\n",
    "        use_itm_head: Optional[bool] = True,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BlipTextVisionModelOutput]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from PIL import Image\n",
    "        >>> import requests\n",
    "        >>> from transformers import AutoProcessor, BlipForImageTextRetrieval\n",
    "\n",
    "        >>> model = BlipForImageTextRetrieval.from_pretrained(\"Salesforce/blip-itm-base-coco\")\n",
    "        >>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip-itm-base-coco\")\n",
    "\n",
    "        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "        >>> image = Image.open(requests.get(url, stream=True).raw)\n",
    "        >>> text = \"an image of a cat\"\n",
    "\n",
    "        >>> inputs = processor(images=image, text=text, return_tensors=\"pt\")\n",
    "        >>> outputs = model(**inputs)\n",
    "        ```\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "\n",
    "        vision_outputs = self.vision_model(\n",
    "            pixel_values=pixel_values,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        image_embeds = vision_outputs[0]\n",
    "        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long)\n",
    "\n",
    "        if use_itm_head:\n",
    "            question_embeds = self.text_encoder(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                encoder_hidden_states=image_embeds,\n",
    "                encoder_attention_mask=image_atts,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "            question_embeds = question_embeds[0] if not return_dict else question_embeds.last_hidden_state\n",
    "\n",
    "            output = self.itm_head(question_embeds[:, 0, :])\n",
    "        else:\n",
    "            question_embeds = self.text_encoder(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "            question_embeds = question_embeds[0] if not return_dict else question_embeds.last_hidden_state\n",
    "\n",
    "            image_feat = normalize(self.vision_proj(image_embeds[:, 0, :]), dim=-1)\n",
    "            text_feat = normalize(self.text_proj(question_embeds[:, 0, :]), dim=-1)\n",
    "\n",
    "            output = image_feat @ text_feat.t()\n",
    "\n",
    "        if not return_dict:\n",
    "            outputs = (output, vision_outputs[0]) + vision_outputs[2:] + (question_embeds,)\n",
    "            return tuple(output for output in outputs if output is not None)\n",
    "\n",
    "        return BlipImageTextMatchingModelOutput(\n",
    "            itm_score=output,\n",
    "            last_hidden_state=vision_outputs.last_hidden_state,\n",
    "            hidden_states=vision_outputs.hidden_states,\n",
    "            attentions=vision_outputs.attentions,\n",
    "            question_embeds=question_embeds,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BlipForImageTextRetrieval.from_pretrained(\"Salesforce/blip-itm-base-coco\")\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-itm-base-coco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steven/miniconda3/envs/tasti/lib/python3.8/site-packages/datasets/load.py:1486: FutureWarning: The repository for visual_genome contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/visual_genome\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "attributes_dataset = load_dataset(\"visual_genome\", \"attributes_v1.2.0\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "image = attributes_dataset[1732]['image']\n",
    "# image = Image.open(path)\"\"\n",
    "text = \"The image is taken in an office\"\n",
    "\n",
    "inputs = processor(images=image, text=text, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[[-0.1864, -0.0696, -0.0550,  ..., -0.8142, -0.4346, -0.4492],\n",
       "          [-0.0113,  0.1055,  0.1201,  ..., -0.6244, -0.2302, -0.2448],\n",
       "          [-0.0550,  0.0763,  0.0909,  ..., -0.6536, -0.2594, -0.2740],\n",
       "          ...,\n",
       "          [-0.6244, -0.5222, -0.5076,  ...,  0.3245,  0.3537,  0.3829],\n",
       "          [-0.6682, -0.6536, -0.7558,  ...,  0.2807,  0.3537,  0.2953],\n",
       "          [-1.0185, -1.0331, -1.0915,  ...,  0.0179,  0.2515,  0.2807]],\n",
       "\n",
       "         [[-0.4614, -0.3414, -0.3414,  ..., -0.4614,  0.0038,  0.0038],\n",
       "          [-0.2813, -0.1613, -0.1613,  ..., -0.2663,  0.1989,  0.1989],\n",
       "          [-0.3264, -0.2063, -0.2213,  ..., -0.2963,  0.1689,  0.1689],\n",
       "          ...,\n",
       "          [-0.8066, -0.6715, -0.6415,  ..., -0.2213, -0.2063, -0.2213],\n",
       "          [-0.8366, -0.8216, -0.8666,  ..., -0.2813, -0.2213, -0.3114],\n",
       "          [-1.1968, -1.1968, -1.2268,  ..., -0.5665, -0.3114, -0.3114]],\n",
       "\n",
       "         [[-0.3995, -0.2857, -0.2857,  ..., -0.0298,  0.5675,  0.7381],\n",
       "          [-0.2431, -0.1435, -0.1435,  ...,  0.1551,  0.7523,  0.9230],\n",
       "          [-0.3142, -0.2004, -0.2004,  ...,  0.1266,  0.7239,  0.8945],\n",
       "          ...,\n",
       "          [-0.5133, -0.4422, -0.5559,  ..., -1.0394, -1.0252, -1.0394],\n",
       "          [-0.5559, -0.5844, -0.7692,  ..., -1.0678, -1.0394, -1.1389],\n",
       "          [-0.9114, -0.9683, -1.0821,  ..., -1.2954, -1.1389, -1.1389]]]],\n",
       "       device='cuda:0'), 'input_ids': tensor([[ 101, 1996, 3746, 2003, 2579, 1999, 2019, 2436,  102]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_embeds = model.text_encoder(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], return_dict=True)[0]\n",
    "question_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_feat = normalize(model.text_proj(question_embeds[:, 0, :]), dim=-1)\n",
    "text_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m embeddings_torch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(embeddings)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 2\u001b[0m image_feat \u001b[38;5;241m=\u001b[39m normalize(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings_torch\u001b[49m\u001b[43m)\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m image_feat\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/miniconda3/envs/tasti/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tasti/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tasti/lib/python3.8/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "embeddings_torch = torch.from_numpy(embeddings).to(device)\n",
    "image_feat = normalize(model.vision_proj(embeddings_torch), dim=-1)\n",
    "image_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tasti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
